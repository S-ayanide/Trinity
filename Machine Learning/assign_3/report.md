# Machine Learning Assignment 3 - Classification

**Dataset IDs:**
- Dataset 1: # id:22--44-22-1
- Dataset 2: # id:22--44--22-1

---

## Introduction

This assignment involves training and evaluating classification models on two datasets with binary labels (+1 and -1). For each dataset, I trained a Logistic Regression classifier with polynomial features and a k-Nearest Neighbors (kNN) classifier. Cross-validation was used to select optimal hyperparameters, and performance was evaluated using confusion matrices and ROC curves, comparing against baseline classifiers.

---

## Dataset 1

### (i)(a) Logistic Regression with Polynomial Features

**Dataset characteristics:**
- 1585 samples
- Class distribution: 528 negative (-1), 1057 positive (+1)
- Imbalanced dataset (~67% positive class)

**Cross-validation setup:**
- 5-fold cross-validation
- Polynomial degrees tested: q ∈ {1, 2, 3, 4, 5}
- Regularization parameter: C ∈ {0.001, 0.01, 0.1, 1, 10, 100, 1000}
- Metric: F1 score

**Results:**
- Best polynomial degree: q = 1
- Best regularization: C = 0.001
- Best CV F1 score: 0.8000

The cross-validation plots show that higher polynomial degrees do not improve performance. In fact, q=1 (linear decision boundary) works best with very strong regularization (C=0.001). This indicates the data is difficult to classify and higher complexity models overfit.

The decision boundary plot shows an almost flat decision that predicts +1 for nearly all points, which is essentially defaulting to the most frequent class due to the strong regularization.

### (i)(b) kNN Classifier

**Cross-validation setup:**
- 5-fold cross-validation
- k values tested: {1, 3, 5, 7, 9, 11, 15, 21, 31, 41, 51}
- Metric: F1 score

**Results:**
- Best k: 51
- Best CV F1 score: 0.8000

The cross-validation plot shows that performance is poor for small k (overfitting) and improves as k increases, eventually plateauing. The large optimal k=51 suggests the data has no clear local structure, and averaging over many neighbors is needed.

The decision boundary shows a relatively smooth boundary between classes, but the large k means the model is quite conservative in its predictions.

### (i)(c) Confusion Matrices

**Logistic Regression:**
```
              Predicted -1  Predicted +1
Actual -1            0            528
Actual +1            0           1057
```
- Accuracy: 0.6669
- True Positive Rate (Recall): 1.0000
- False Positive Rate: 1.0000
- Precision: 0.6669

The model predicts +1 for all samples. This achieves 66.69% accuracy simply because 66.69% of the data is positive.

**kNN:**
```
              Predicted -1  Predicted +1
Actual -1            1            527
Actual +1            0           1057
```
- Accuracy: 0.6675
- True Positive Rate: 1.0000
- False Positive Rate: 0.9981
- Precision: 0.6673

Nearly identical behavior - predicts +1 for almost all samples with only 1 negative prediction.

**Baseline (Most Frequent):**
```
              Predicted -1  Predicted +1
Actual -1            0            528
Actual +1            0           1057
```
- Accuracy: 0.6669

Identical to Logistic Regression - always predicts +1.

**Baseline (Random):**
```
              Predicted -1  Predicted +1
Actual -1          253            275
Actual +1          543            514
```
- Accuracy: 0.4839

Random predictions achieve ~48% accuracy as expected.

**Confusion matrix calculation:**
The confusion matrix is calculated by:
1. Using the trained model to predict labels for all training samples
2. Comparing predicted labels against true labels
3. Counting:
   - True Negatives (TN): Correctly predicted -1
   - False Positives (FP): Predicted +1 but actually -1
   - False Negatives (FN): Predicted -1 but actually +1
   - True Positives (TP): Correctly predicted +1

Matrix layout:
```
              Predicted -1  Predicted +1
Actual -1         TN            FP
Actual +1         FN            TP
```

### (i)(d) ROC Curves

**ROC curve explanation:**
The ROC curve is generated by:
1. For classifiers that output probability/confidence scores (Logistic Regression: decision_function, kNN: predict_proba)
2. Vary the decision threshold from -∞ to +∞
3. For each threshold, calculate:
   - True Positive Rate = TP / (TP + FN)
   - False Positive Rate = FP / (FP + TN)
4. Plot TPR vs FPR
5. Calculate AUC (Area Under Curve)

For baseline classifiers that output only hard labels (not probabilities), we plot single points representing their fixed TPR and FPR.

**Results:**
- Logistic Regression AUC: 0.5045
- kNN AUC: 0.5686
- Baseline (Most Frequent): Single point at (1.0, 1.0) - predicts all +1
- Baseline (Random): Single point around (0.5, 0.5) - random guessing

The ROC curves show both classifiers barely perform better than random chance (AUC ≈ 0.5). The diagonal dashed line represents a random classifier. Both curves hug this line closely, indicating poor discriminative ability.

### (i)(e) Performance Comparison and Evaluation

**Key observations:**

1. **Dataset difficulty:** This is a very hard dataset to predict. Both trained classifiers achieve AUC barely above 0.5, indicating the features provide almost no discriminative power for classification.

2. **Comparison with baselines:**
   - Both Logistic Regression and kNN perform only marginally better than the most-frequent baseline (66.69% vs 66.69% accuracy)
   - They are significantly better than random guessing (66-67% vs 48%)
   - However, the small margin above baseline suggests the data is nearly useless for prediction

3. **Classifier comparison:**
   - kNN (AUC=0.5686) slightly outperforms Logistic Regression (AUC=0.5045)
   - However, the difference is minimal and both are close to random performance
   - Neither classifier is significantly better than the other

4. **ROC curve interpretation:**
   - The ROC curves stay very close to the 45° diagonal line
   - An ideal classifier would have a curve in the top-left corner (0, 1)
   - Our classifiers are far from ideal, confirming this dataset is extremely difficult

5. **Practical recommendation:**
   - **Neither classifier should be used in practice** for this dataset
   - The features measured do not capture the relationships needed for classification
   - If forced to choose, kNN with k=51 is marginally better, but still essentially guessing
   - Better features or more data would be needed to make meaningful predictions

6. **Why is this dataset so hard?**
   - Looking at the scatter plot, the two classes likely overlap significantly in feature space
   - There's probably no clear decision boundary that can separate them
   - The data is inherently noisy or the measured features are not predictive

---

## Dataset 2

### (ii)(a) Logistic Regression with Polynomial Features

**Dataset characteristics:**
- 1079 samples
- Class distribution: 804 negative (-1), 275 positive (+1)
- Highly imbalanced dataset (~74.5% negative class)

**Cross-validation setup:**
- 5-fold cross-validation
- Polynomial degrees tested: q ∈ {1, 2, 3, 4, 5}
- Regularization parameter: C ∈ {0.001, 0.01, 0.1, 1, 10, 100, 1000}
- Metric: F1 score

**Results:**
- Best polynomial degree: q = 2
- Best regularization: C = 100
- Best CV F1 score: 0.9508

The cross-validation plots show that q=2 (quadratic decision boundary) performs best with weak regularization (C=100). This indicates the data has a nonlinear but not overly complex structure. Higher degrees don't improve performance, suggesting a quadratic boundary is sufficient.

The decision boundary plot shows a clear curved boundary that effectively separates the two classes.

### (ii)(b) kNN Classifier

**Cross-validation setup:**
- 5-fold cross-validation
- k values tested: {1, 3, 5, 7, 9, 11, 15, 21, 31, 41, 51}
- Metric: F1 score

**Results:**
- Best k: 7
- Best CV F1 score: 0.9498

The cross-validation plot shows good performance across a range of k values with optimal at k=7. The small optimal k indicates the data has clear local structure - nearby points tend to have the same label. Performance degrades slightly for very small k (overfitting) and for very large k (oversmoothing).

The decision boundary shows a complex nonlinear boundary that captures local patterns in the data.

### (ii)(c) Confusion Matrices

**Logistic Regression:**
```
              Predicted -1  Predicted +1
Actual -1          794            10
Actual +1           14           261
```
- Accuracy: 0.9778
- True Positive Rate: 0.9491
- False Positive Rate: 0.0124
- Precision: 0.9631

Excellent performance: correctly classifies 794/804 negative samples and 261/275 positive samples. Only 24 total errors out of 1079 samples.

**kNN:**
```
              Predicted -1  Predicted +1
Actual -1          798             6
Actual +1           14           261
```
- Accuracy: 0.9815
- True Positive Rate: 0.9491
- False Positive Rate: 0.0075
- Precision: 0.9775

Slightly better than Logistic Regression: correctly classifies 798/804 negative samples and 261/275 positive samples. Only 20 total errors.

**Baseline (Most Frequent):**
```
              Predicted -1  Predicted +1
Actual -1          804             0
Actual +1          275             0
```
- Accuracy: 0.7451

Always predicts -1 (the most frequent class), achieving 74.51% accuracy but missing all positive samples.

**Baseline (Random):**
```
              Predicted -1  Predicted +1
Actual -1          402           402
Actual +1          139           136
```
- Accuracy: 0.4986

Random predictions achieve ~50% accuracy as expected.

### (ii)(d) ROC Curves

**Results:**
- Logistic Regression AUC: 0.9982
- kNN AUC: 0.9977
- Baseline (Most Frequent): Single point at (0.0, 0.0) - predicts all -1
- Baseline (Random): Single point around (0.5, 0.5) - random guessing

The ROC curves show both classifiers perform excellently, with curves hugging the top-left corner. AUC values near 1.0 indicate near-perfect discrimination between classes. The curves are far from the random classifier diagonal line.

### (ii)(e) Performance Comparison and Evaluation

**Key observations:**

1. **Dataset difficulty:** This is an easy dataset to predict. Both trained classifiers achieve AUC near 1.0, indicating the features provide excellent discriminative power.

2. **Comparison with baselines:**
   - Both Logistic Regression (97.78%) and kNN (98.15%) vastly outperform the most-frequent baseline (74.51%)
   - They are far superior to random guessing (49.86%)
   - The large improvement over baseline demonstrates the models learned meaningful patterns

3. **Classifier comparison:**
   - kNN (AUC=0.9977, Acc=98.15%) slightly outperforms Logistic Regression (AUC=0.9982, Acc=97.78%)
   - However, the difference is minimal - both are excellent
   - Logistic Regression has slightly higher AUC, while kNN has slightly higher accuracy
   - The differences are so small they're practically equivalent

4. **ROC curve interpretation:**
   - Both curves are very close to the ideal top-left corner (0, 1)
   - This indicates the classifiers can achieve high TPR with very low FPR
   - The curves stay well above the 45° diagonal, confirming excellent performance

5. **Practical recommendation:**
   - **Either classifier would work excellently** for this dataset
   - If forced to choose:
     - **kNN (k=7)** has marginally better accuracy (98.15% vs 97.78%) and slightly lower false positive rate
     - **Logistic Regression (q=2, C=100)** is more interpretable and faster to deploy
   - For production use, I would recommend **Logistic Regression** because:
     - Nearly identical performance to kNN
     - More interpretable (can examine coefficients)
     - Faster inference (no need to search through training data)
     - Easier to deploy and maintain

6. **Why is this dataset so easy?**
   - The two classes are well-separated in feature space
   - A simple quadratic boundary is sufficient to separate them
   - The measured features clearly capture the relevant relationships
   - Low noise and clear patterns make classification straightforward

---

## Comparison Between Datasets

**Dataset 1:**
- Hard to predict (AUC ≈ 0.5-0.6)
- Classifiers barely outperform baseline
- Features don't capture discriminative information
- ROC curves close to diagonal
- Not suitable for practical use

**Dataset 2:**
- Easy to predict (AUC ≈ 0.99)
- Classifiers vastly outperform baseline
- Features capture excellent discriminative information
- ROC curves close to ideal (0, 1) point
- Excellent for practical deployment

This demonstrates that **not all datasets are useful for machine learning**. Even with sophisticated algorithms and careful hyperparameter tuning, Dataset 1 remains unpredictable because the measured features simply don't contain the information needed for classification. Dataset 2, by contrast, has features that perfectly capture the relationships needed, making it easy to achieve excellent performance.

---

## Technical Notes

**Hyperparameter Selection:**
All hyperparameters were selected using 5-fold cross-validation with F1 score as the metric. F1 score is appropriate for these imbalanced datasets as it balances precision and recall.

**Baseline Classifiers:**
1. Most Frequent: Always predicts the most common class in training data
2. Random: Makes random predictions with uniform probability

**Performance Metrics:**
- Accuracy: (TP + TN) / (TP + TN + FP + FN)
- True Positive Rate (Recall): TP / (TP + FN)
- False Positive Rate: FP / (FP + TN)
- Precision: TP / (TP + FP)
- F1 Score: 2 × (Precision × Recall) / (Precision + Recall)
- AUC: Area under the ROC curve, ranges from 0 to 1, with 0.5 being random and 1.0 being perfect

---

## Code

All code is in `assignment3.py` which can be run with:
```bash
python assignment3.py
```

The code follows the structure from the lecture notes, using:
- `sklearn.linear_model.LogisticRegression` with L2 penalty
- `sklearn.neighbors.KNeighborsClassifier`
- `sklearn.preprocessing.PolynomialFeatures`
- `sklearn.model_selection.KFold` for cross-validation
- `sklearn.metrics` for evaluation metrics

